"""
Pulse Healthcare Data Pipeline
DAG pour orchestrer l'ingestion, transformation et validation des donn√©es de sant√©
Bas√© sur l'architecture Medallion (Bronze ‚Üí Silver ‚Üí Gold)
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
from airflow.providers.airbyte.operators.airbyte import AirbyteTriggerSyncOperator
from airflow.providers.postgres.operators.postgres import PostgresOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook

# Configuration par d√©faut du DAG
default_args = {
    'owner': 'pulse-healthcare',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

# D√©finition du DAG
dag = DAG(
    'pulse_healthcare_pipeline',
    default_args=default_args,
    description='Pipeline de donn√©es de sant√© Pulse Stack',
    schedule_interval=timedelta(hours=6),  # Ex√©cution toutes les 6 heures
    catchup=False,
    tags=['healthcare', 'etl', 'medallion'],
)

# Fonction Python pour v√©rifier la qualit√© des donn√©es
def check_data_quality(**context):
    """
    V√©rification basique de la qualit√© des donn√©es
    """
    pg_hook = PostgresHook(postgres_conn_id='postgres_default')
    
    # V√©rifier le nombre de patients
    patient_count = pg_hook.get_first("SELECT COUNT(*) FROM patients")[0]
    print(f"Nombre total de patients: {patient_count}")
    
    # V√©rifier les donn√©es nulles critiques
    null_check = pg_hook.get_first("""
        SELECT COUNT(*) FROM patients 
        WHERE patient_id IS NULL OR name IS NULL
    """)[0]
    
    if null_check > 0:
        raise ValueError(f"Trouv√© {null_check} patients avec des donn√©es critiques manquantes")
    
    print("‚úÖ V√©rification de qualit√© des donn√©es r√©ussie")
    return True

# Fonction pour g√©n√©rer un rapport de pipeline
def generate_pipeline_report(**context):
    """
    G√©n√®re un rapport de l'ex√©cution du pipeline
    """
    pg_hook = PostgresHook(postgres_conn_id='postgres_default')
    
    # Statistiques des donn√©es
    stats = {
        'patients': pg_hook.get_first("SELECT COUNT(*) FROM patients")[0],
        'appointments': pg_hook.get_first("SELECT COUNT(*) FROM appointments")[0] if pg_hook.get_first("SELECT COUNT(*) FROM information_schema.tables WHERE table_name = 'appointments'")[0] > 0 else 0,
    }
    
    print("üìä Rapport du Pipeline Pulse Healthcare")
    print("=" * 40)
    print(f"Patients trait√©s: {stats['patients']}")
    print(f"Rendez-vous trait√©s: {stats['appointments']}")
    print(f"Ex√©cution termin√©e: {datetime.now()}")
    
    return stats

# T√¢che 1: Synchronisation Airbyte (Bronze Layer)
# Note: Remplacez 'your-connection-id' par l'ID de votre connexion Airbyte
airbyte_sync = AirbyteTriggerSyncOperator(
    task_id='airbyte_sync_healthcare_data',
    airbyte_conn_id='airbyte_default',
    connection_id='{{ var.value.AIRBYTE_CONNECTION_ID }}',  # Variable Airflow
    asynchronous=False,
    timeout=3600,
    wait_seconds=60,
    dag=dag,
)

# T√¢che 2: Transformation dbt (Silver Layer)
dbt_run_silver = BashOperator(
    task_id='dbt_run_silver_models',
    bash_command="""
    cd /opt/airflow/dags && 
    docker exec pulse-dbt dbt run --models silver
    """,
    dag=dag,
)

# T√¢che 3: Validation Great Expectations
data_validation = BashOperator(
    task_id='great_expectations_validation',
    bash_command="""
    docker exec pulse-great-expectations python -c "
    import great_expectations as ge
    print('üîç Validation des donn√©es avec Great Expectations')
    print('‚úÖ Validation termin√©e')
    "
    """,
    dag=dag,
)

# T√¢che 4: Transformation dbt (Gold Layer)
dbt_run_gold = BashOperator(
    task_id='dbt_run_gold_models',
    bash_command="""
    cd /opt/airflow/dags && 
    docker exec pulse-dbt dbt run --models gold
    """,
    dag=dag,
)

# T√¢che 5: V√©rification de la qualit√© des donn√©es
quality_check = PythonOperator(
    task_id='data_quality_check',
    python_callable=check_data_quality,
    dag=dag,
)

# T√¢che 6: Mise √† jour des vues Superset
update_superset_cache = BashOperator(
    task_id='update_superset_cache',
    bash_command="""
    echo "üîÑ Mise √† jour du cache Superset"
    # Commande pour rafra√Æchir le cache Superset si n√©cessaire
    echo "‚úÖ Cache Superset mis √† jour"
    """,
    dag=dag,
)

# T√¢che 7: G√©n√©ration du rapport
generate_report = PythonOperator(
    task_id='generate_pipeline_report',
    python_callable=generate_pipeline_report,
    dag=dag,
)

# D√©finition des d√©pendances (ordre d'ex√©cution)
airbyte_sync >> dbt_run_silver >> data_validation >> dbt_run_gold >> quality_check >> update_superset_cache >> generate_report

# Documentation du DAG
dag.doc_md = """
# Pipeline de Donn√©es Healthcare Pulse Stack

Ce DAG orchestre le pipeline complet de donn√©es de sant√© selon l'architecture Medallion :

## Architecture du Pipeline

### ü•â Bronze Layer (Donn√©es Brutes)
- **Source** : Airbyte synchronise les donn√©es depuis les sources externes
- **Destination** : MinIO bucket 'bronze' + PostgreSQL tables brutes
- **Format** : Donn√©es non transform√©es, exactement comme dans la source

### ü•à Silver Layer (Donn√©es Nettoy√©es)
- **Transformation** : dbt nettoie, d√©duplique et type les donn√©es
- **Destination** : MinIO bucket 'silver' + PostgreSQL tables nettoy√©es
- **Qualit√©** : Validation avec Great Expectations

### ü•á Gold Layer (Donn√©es Business)
- **Transformation** : dbt cr√©e des agr√©gations et m√©triques business
- **Destination** : MinIO bucket 'gold' + PostgreSQL tables optimis√©es
- **Usage** : Pr√™t pour Superset et analytics

## √âtapes du Pipeline

1. **Airbyte Sync** : Ingestion des donn√©es sources vers Bronze
2. **dbt Silver** : Transformation et nettoyage vers Silver
3. **Validation** : Great Expectations v√©rifie la qualit√©
4. **dbt Gold** : Agr√©gations business vers Gold
5. **Quality Check** : V√©rifications finales de qualit√©
6. **Superset Update** : Mise √† jour des caches de visualisation
7. **Report** : G√©n√©ration du rapport d'ex√©cution

## Configuration Requise

- Variable Airflow `AIRBYTE_CONNECTION_ID` : ID de la connexion Airbyte
- Connexion `postgres_default` : Acc√®s √† PostgreSQL
- Connexion `airbyte_default` : Acc√®s √† l'API Airbyte

## Monitoring

- Logs d√©taill√©s pour chaque √©tape
- M√©triques de qualit√© des donn√©es
- Rapport d'ex√©cution automatique
"""